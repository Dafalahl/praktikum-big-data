{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "014a640f-52f5-4cf1-88ea-7bc0e35ce156",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/29 08:16:43 WARN Utils: Your hostname, dafa-Aspire-A514-55G resolves to a loopback address: 127.0.1.1; using 192.168.100.27 instead (on interface wlp43s0)\n",
      "25/11/29 08:16:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/29 08:16:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/11/29 08:16:49 WARN Instrumentation: [f578dd9c] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/11/29 08:16:51 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "25/11/29 08:16:51 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "25/11/29 08:16:51 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.9999999999999992]\n",
      "Intercept: 15.000000000000009\n"
     ]
    }
   ],
   "source": [
    "# Example: Linear Regression with Spark MLlib\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName('MLlib Example').getOrCreate()\n",
    "\n",
    "# Load sample data\n",
    "data = [(1, 5.0, 20.0), (2, 10.0, 25.0), (3, 15.0, 30.0), (4, 20.0, 35.0)]\n",
    "columns = ['ID', 'Feature', 'Target']\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Prepare data for modeling\n",
    "assembler = VectorAssembler(inputCols=['Feature'], outputCol='Features')\n",
    "df_transformed = assembler.transform(df)\n",
    "\n",
    "# Train a linear regression model\n",
    "lr = LinearRegression(featuresCol='Features', labelCol='Target')\n",
    "model = lr.fit(df_transformed)\n",
    "\n",
    "# Print model coefficients\n",
    "print(f'Coefficients: {model.coefficients}')\n",
    "print(f'Intercept: {model.intercept}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7aa1c1f8-056d-48b9-9617-4d063450c6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [-12.262057937838394,4.087352269372807]\n",
      "Intercept: 11.568912735310269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/29 08:16:58 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "# Practice: Logistic Regression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "data = [\n",
    "    (1, Vectors.dense([2.0, 3.0]), 0),\n",
    "    (2, Vectors.dense([1.0, 5.0]), 1),\n",
    "    (3, Vectors.dense([2.5, 4.5]), 1),\n",
    "    (4, Vectors.dense([3.0, 6.0]), 0)\n",
    "]\n",
    "\n",
    "columns = ['ID', 'Features', 'Label']\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "lr = LogisticRegression(featuresCol='Features', labelCol='Label')\n",
    "model = lr.fit(df)\n",
    "\n",
    "print(\"Coefficients:\", model.coefficients)\n",
    "print(\"Intercept:\", model.intercept)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f01ec74-ef61-4adc-86de-adae2db88c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centers: [array([12.5, 12.5]), array([3., 3.])]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "data = [\n",
    "    (1, 1.0, 1.0),\n",
    "    (2, 5.0, 5.0),\n",
    "    (3, 10.0, 10.0),\n",
    "    (4, 15.0, 15.0)\n",
    "]\n",
    "columns = ['ID', 'x', 'y']\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "assembler = VectorAssembler(inputCols=['x', 'y'], outputCol='Features')\n",
    "df_vec = assembler.transform(df)\n",
    "\n",
    "kmeans = KMeans(featuresCol='Features', k=2)\n",
    "model = kmeans.fit(df_vec)\n",
    "\n",
    "centers = model.clusterCenters()\n",
    "print(f'Cluster Centers: {centers}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2580aab-44fc-4ab1-be78-95c399652cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----------+\n",
      "|label|prediction|probability|\n",
      "+-----+----------+-----------+\n",
      "|0.0  |0.0       |[1.0,0.0]  |\n",
      "|0.0  |0.0       |[1.0,0.0]  |\n",
      "|0.0  |0.0       |[1.0,0.0]  |\n",
      "|0.0  |0.0       |[1.0,0.0]  |\n",
      "|0.0  |0.0       |[1.0,0.0]  |\n",
      "|1.0  |1.0       |[0.0,1.0]  |\n",
      "|1.0  |1.0       |[0.0,1.0]  |\n",
      "|0.0  |0.0       |[1.0,0.0]  |\n",
      "|1.0  |1.0       |[0.0,1.0]  |\n",
      "|1.0  |1.0       |[0.0,1.0]  |\n",
      "+-----+----------+-----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Accuracy (Before Tuning): 0.9651162790697675\n",
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|  1.0|       1.0|   36|\n",
      "|  0.0|       1.0|    2|\n",
      "|  1.0|       0.0|    1|\n",
      "|  0.0|       0.0|   47|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/29 10:14:51 WARN BlockManager: Block rdd_599_0 already exists on this machine; not re-adding it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy After Cross-Validation: 1.0\n"
     ]
    }
   ],
   "source": [
    "# ======================================================\n",
    "# 1. Load Data\n",
    "# ======================================================\n",
    "df = spark.read.csv(\n",
    "    \"data.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "# Drop kolom tidak dipakai\n",
    "df = df.drop(\"_c32\")\n",
    "\n",
    "# Label Encoding (diagnosis -> label)\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"diagnosis\", outputCol=\"label\")\n",
    "df = indexer.fit(df).transform(df)\n",
    "\n",
    "# Vector Assembler (fitur -> features)\n",
    "feature_cols = [c for c in df.columns if c not in (\"id\", \"diagnosis\", \"label\")]\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "df = assembler.transform(df)\n",
    "\n",
    "# Trainâ€“test split\n",
    "train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# ======================================================\n",
    "# 2 Build & Evaluate Model\n",
    "# ======================================================\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "model = lr.fit(train_df)\n",
    "\n",
    "predictions = model.transform(test_df)\n",
    "predictions.select(\"label\", \"prediction\", \"probability\").show(10, truncate=False)\n",
    "\n",
    "# Evaluasi akurasi\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy (Before Tuning):\", accuracy)\n",
    "\n",
    "# Confusion Matrix\n",
    "predictions.groupBy(\"label\", \"prediction\").count().show()\n",
    "\n",
    "# ======================================================\n",
    "# 3 Hyperparameter Tuning + Cross-Validation\n",
    "# ======================================================\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Model dasar\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "# Variasi parameter\n",
    "paramGrid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 1.0])        # Regularization\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])  # L2 - mix - L1\n",
    "    .build()\n",
    ")\n",
    "\n",
    "# CrossValidator\n",
    "cv = CrossValidator(\n",
    "    estimator=lr,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=5,\n",
    "    parallelism=2\n",
    ")\n",
    "\n",
    "# Train model dengan cross-validation\n",
    "cv_model = cv.fit(train_df)\n",
    "\n",
    "# Evaluasi setelah tuning\n",
    "pred_cv = cv_model.transform(test_df)\n",
    "accuracy_cv = evaluator.evaluate(pred_cv)\n",
    "\n",
    "print(\"Accuracy After Cross-Validation:\", accuracy_cv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dbc1f3-1565-48a0-9c73-aa4748bd25fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
